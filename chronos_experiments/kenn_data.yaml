data:
  dataset_paths:
    train: ../datasets/ren_et_al/bbn/train.json
    dev: ../datasets/ren_et_al/bbn/dev.json
    test: ../datasets/ren_et_al/bbn/test-12k.json
  # train_test_mapping:
  #   livingthing: living_thing
  dataset_reader_params:
    name : BaseDataset
  tokenizer_params:
    name : BaseBERTTokenizedDataset
    bertlike_model_name: distilbert-base-uncased
    max_mention_words: 5
    max_right_words: 13
    max_left_words: 13
    max_tokens: 80
  dataset_params:
    name: ET_Dataset
  dataloader_params:
    name: torch.DataLoader
    train:
      batch_size: 64
      shuffle: True
    dev:
      batch_size: 64
    test:
      batch_size: 64
  rw_options:
    modality: CreateAndSave # in [Create, CreateAndSave, Load]
    dirpath: ../tokenized_datasets/bbn/dummy_tokenized 
    light: True
    type2id_file_path: ../datasets/datasets_for_incremental_training/figer/figer_all_types_no_location_sons.txt